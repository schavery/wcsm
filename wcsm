#!/usr/bin/env python3

"""
wcsm stands for Web Change Stop Motion
This script is designed to watch your directory for file modifications
and will then make a request to the uris you define to save those modifications
to html files.
Then, once you've done your work and saved a bunch of flat files, you can
render these captures as images and then stitch them together with ffmpeg
to create a little movie of the creation of a website!

Summer 2014
Steve Avery
MIT license.
"""

import argparse
import base64
import chardet
import cssutils
import glob
import hashlib
import imghdr
import logging
import os
import re
import requests
import sys
import time

from bs4 import BeautifulSoup
from os import path as op
from urllib.parse import urlsplit, urlunsplit, urljoin
from watchdog.events import RegexMatchingEventHandler
from watchdog.observers import Observer


# TODOS:

# better docs - include python version required
# examples
# second level of verbose? print out inliner info
# some changes are invisible; want to throw these away
# do initial get before change caught, but only when no file yet exists?



# output dir structure
# <outdir>
#		/uri[0]
#			/time0.html
#			/time1.html
#			...
#		/uri[1]
#			...
#		...


class Http(object):
	""" manage the http requests """

	def req(self, uri):
		htreq = requests.get(uri)
		if htreq.status_code == 200:
			if htreq.encoding == None:
				enc = chardet.detect(htreq.content)['encoding']
				if enc != None:
					try:
						return htreq.content.decode(enc).encode('utf-8')
					except UnicodeDecodeError:
						return htreq.content
				else:
					# give up
					return htreq.content

			return htreq.content.decode(htreq.encoding).encode('utf-8')
		else:
			print('Net error:\nStatus: ' + str(htreq.status_code) + '\n' \
					 + 'Requested URI: ' + uri)



class WebGetter(object):
	""" fetch the contents of a url using WebGetter.graball() """

	def __init__(self, options, http):
		# set up debounce interval in millis
		# might need fine tuning for different applications
		self.debounce_interval = 1000
		self.thistime = 0
		self.lasttime = 0
		self.debug = options.verbose

		# no duplicate uris
		uniqueset = set(options.fetch)
		self.urilist = list(uniqueset)

		# find or create directories for output
		self.outdirname = op.abspath(options.output)
		for uri in self.urilist:

			name = self.pathfromuri(uri)
			outpath = op.normpath(self.outdirname + '/' + name)
			if not op.exists(outpath):
				os.makedirs(outpath)

		# prepare urls for fetching.
		self.http = http

		# make sure we have a net scheme
		newlist = []
		for uri in self.urilist:
			if urlsplit(uri).scheme == '':
				newlist.append('http://' + uri)
			else:
				newlist.append(uri)

		self.urilist = newlist

		# check for any existing files and populate the initial version of the uris
		for uri in self.urilist:
			uri_dir = op.normpath(self.outdirname + '/' + self.pathfromuri(uri))
			if os.listdir(uri_dir) == []:
				self.grabone(uri)


		# are we good to go now?
		print('Ready to watch...')


	def debounce(self):
		""" don't go fetching pages too frequently """
		self.thistime = int(time.time() * 1000)
		if (self.thistime - self.lasttime) > self.debounce_interval:
			self.lasttime = self.thistime
			return True
		else:
			return False

	@staticmethod
	def pathfromuri(uri):
		""" transform the uri into a simple directory name """

		address = urlsplit(uri)
		base = address.netloc

		if address.path != '':

			# remove first slash
			if base == '' and address.path[0:1] == '/':
				path = address.path[1:]
			else:
				path = address.path

			# don't underscore a directory type path
			if path[-1] == '/':
				path = re.sub('/', '_', path[:-1])
			else:
				path = re.sub('/', '_', path)

			base += path

		if address.query != '':
			query = re.sub('&', '-', address.query)
			base += '+' + query

		return base


	def checkhash(self, checkdir, flatcontent):
		""" returns a boolean that indicates whether the last
			file saved has the same hash value as the fetched content """

		thishash = hashlib.md5(flatcontent.encode('utf-8')).hexdigest()
		oldfiles = [f for f in os.listdir(checkdir) if op.isfile(f)]
		goahead = False

		if len(oldfiles) > 0:
			oldfiles.sort()
			mostrecent = oldfiles[-1]
			fcontents = open(op.normpath(checkdir + '/' + mostrecent), 'rb')
			pasthash = hashlib.md5(fcontents.read()).hexdigest()
			if pasthash != thishash:
				goahead = True
		else:
			# no files exist yet
			goahead = True

		return goahead


	def grabone(self, uri):
		name = str(time.time()).split('.')[0] + '.html'
		basepath = op.normpath(self.outdirname + '/' + WebGetter.pathfromuri(uri)  + '/')

		req = self.http.req(uri)
		print('Fetching', uri)

		flat = Inliner(req, uri, self.http, self.outdirname).get()

		if self.checkhash(basepath, flat):
			uridir = WebGetter.pathfromuri(uri)
			path = self.outdirname + '/' + uridir + '/' + name
			fhandle = open(op.normpath(path), 'wb')
			fhandle.write(flat.encode('utf-8'))
			fhandle.close()
			print('Wrote', path)
		else:
			print('Not writing, hash matches last written file')


	def graball(self):
		""" handles the logic of grabbing a webpage
			and flattening it to a single file,
			as well as preventing too frequent requests and
			superfluous output """

		if self.debug:
			if self.debounce():
				for uri in self.urilist:
					self.grabone(uri)
					
			else:
				print('Not fetching because we fetched within the last second.')

			print('Waiting...')

		else:
			if self.debounce():
				name = str(time.time()).split('.')[0] + '.html'
				for uri in self.urilist:
					basepath = op.normpath(self.outdirname + '/' + WebGetter.pathfromuri(uri)  + '/')

					req = self.http.req(uri)
					flat = Inliner(req, uri, self.http).get()

					if self.checkhash(basepath, flat):
						uridir = WebGetter.pathfromuri(uri)
						path = self.outdirname + '/' + uridir + '/' + name
						fhandle = open(op.normpath(path), 'wb')
						fhandle.write(flat.encode('utf-8'))
						fhandle.close()



class Inliner(object):
	""" take in html request data, and modify it so that
		it doesn't have any external references """

	def __init__(self, htmlreq, uri, http, outputdir):
		self.uri = uri
		self.http = http
		self.soup = BeautifulSoup(htmlreq, 'lxml')
		self.outputdir = outputdir

		self.inlimg()
		self.inlscript()
		self.inlcss()


	def get(self):
		return self.soup.prettify()


	def getexternalresource(self, uri, path):
		""" handle arbitrary paths, and return the resource data """
		absolute = urljoin(uri, path)
		return self.http.req(absolute)


	def base64it(self, raw, ref):
		""" convert a bytestream or xml file into base64 data uri """

		imgtype = imghdr.what('ignored', raw)
		if imgtype == None:
			imgtype = ref.split('.').pop()
			if imgtype == 'svg':
				imgtype += '+xml'
		
		img64 = base64.b64encode(raw)
		return 'data:image/' + imgtype + ';base64,' + img64.decode('ascii')


	def inlimg(self):
		""" images become base64 encoded as data uris """

		imglist = self.soup.select('img')
		for img in imglist:
			if hasattr(img, 'src'):
				ref = img['src']
				srcuri = urlsplit(ref)

				# test for data uri already
				if srcuri.scheme == 'data':
					continue

				imgraw = self.getexternalresource(self.uri, ref)
				img['src'] = self.base64it(imgraw, ref)


	def inlscript(self):
		""" get the scripts coming from this domain only
			we don't care about external libraries """

		domain = urlsplit(self.uri).netloc
		scriptlist = self.soup.select('script')
		for script in scriptlist:
			try:
				ref = getattr(script, 'src')
				parsedref = urlsplit(ref)
				if domain == parsedref.netloc \
					or parsedref.netloc == '':

					scriptcontents = self.getexternalresource(self.uri, ref)
					del script['src']
					script.string = scriptcontents
			except AttributeError:
				pass


	def inlcss(self):
		""" handle @import and url ref
			handle style attributes with url ref
			change links with rel stylesheet to style tag """

		cssutils.log.setLevel(logging.FATAL)

		linklist = self.soup.select('link[rel="stylesheet"]')
		domain = urlsplit(self.uri).netloc

		for link in linklist:
			if hasattr(link, 'href'):
				self.ref = link['href']

				print('Fetching CSS from', self.ref)

				# make sure its a local stylesheet
				hrefdomain = urlsplit(self.ref).netloc
				if domain == hrefdomain \
					or hrefdomain == '':

					# fetch the linked stylesheet
					sty = self.getexternalresource(self.ref, self.ref)
					sty = cssutils.parseString(sty)
					
					# flatten out any imports into the same stylesheet
					sty = cssutils.resolveImports(sty)

					# remove any referenced uris
					cssutils.replaceUrls(sty, self.cssurihandler, ignoreImportRules=True)

					# create the new destination style tag
					newstyle = self.soup.new_tag('style')

					# insert our results
					newstyle.string = sty.cssText.decode()

					# before the old link tag
					link.insert_before(newstyle) 

					# remove the stylesheet link
					link.decompose()

				else:
					print('Not fetching external CSS')

		self.ref = None
		styleattrs = self.soup.select('[style]')
		for tag in styleattrs:
			sty = cssutils.parseStyle(tag['style'])
			sty = cssutils.replaceUrls(sty, self.cssurihandler)
			tag['style'] = sty.cssText


	def cssurihandler(self, uri):
		""" take a url() from some css and replace the url with a path to
			a local resource """

		# ref is the path from which we need to normalize the uri
		if self.ref == None:
			base = self.uri
		else:
			base = self.ref

		# we can limit this to only onsite resources, as usual.
		parsed_url = urlsplit(urljoin(base, uri))
		hrefdomain = parsed_url.netloc

		if urlsplit(self.uri).netloc == hrefdomain or hrefdomain == '':
			file_resource = self.getexternalresource(parsed_url.geturl(), '')
			md5_file_resource = hashlib.md5(file_resource).hexdigest()

			# split into path and extension
			parsed_path = op.splitext(parsed_url.path)
			server_path = parsed_path[0]
		
			# path in which we will try to match for versions of this filename
			# but only with the same file extension
			resource_folder = op.normpath(self.outputdir + '/' + WebGetter.pathfromuri(self.uri) + '/res/')
			search_path = resource_folder + WebGetter.pathfromuri(server_path) + '*' + parsed_path[1][1:]

			# get a potentially empty list of matches
			potential_matches = glob.iglob(search_path)

			# a flag for determining whether we need to save this file or not
			file_match_found = False
			file_match_count = 0

			for file_match in potential_matches:
				file_match_contents = open(file_match, 'rb')
				file_match_count += 1

				if hashlib.md5(file_match_contents.read()).hexdigest() == md5_file_resource:
					# we have found a matching resource file
					file_match_contents.close()
					file_match_found = True
					break
				else:
					file_match_contents.close()

			if file_match_found:
				# convert file_match path to relative url and return it
				return op.relpath(op.normpath(resource_folder + '/' + file_match), start=self.outputdir)
			else:
				if file_match_count == 0:
					# filename does not need a version indicator
					full_path = op.normpath(resource_folder + '/' + WebGetter.pathfromuri(parsed_url.path))
					
				else:
					# filename is filepath.<file_match_count>.extension
					full_path = op.normpath(
							resource_folder
							+ '/'
							+ WebGetter.pathfromuri(server_path)
							+ '.'
							+ str(file_match_count + 1)
							+ parsed_path[1]) # includes the .


				# save file contents and return relative path
				if not op.exists(resource_folder):
					os.makedirs(resource_folder)

				fhandle = open(full_path, 'wb')
				fhandle.write(file_resource)
				fhandle.close()
				relative_path = op.relpath(full_path, start=op.normpath(resource_folder + '/..'))
				return relative_path

		else:
			# the reference is external, so just use the existing uri
			return uri



class EventHandler(RegexMatchingEventHandler):
	def __init__(self, wg, options):
		super(EventHandler, self).__init__(options.regex)
		self.ignore = op.abspath(options.output)
		self.webgetter = wg


	def on_any_event(self, event):
		# don't do anything for events that are in the output dir
		emittedpath = op.split(event.src_path)
		while len(emittedpath[0]) >= len(self.ignore):
			if emittedpath[0] != self.ignore:
				emittedpath = op.split(emittedpath[0])
			else:
				return

		self.webgetter.graball()



if __name__ == '__main__':
	parser = argparse.ArgumentParser(description='Capture updates to a web site as you edit the files.',
										epilog='Consider using a & after the command to run it in the background.' \
												+ '\nExample: ' + sys.argv[0] + ' <uri> &')
	parser.add_argument('-v', '--verbose', action='store_true',
						help='Print lots of extra info')

	parser.add_argument('-i', '--input', metavar='<dir>', default=os.getcwd(),
						help='The directory to watch for file changes. Defaults to PWD')
	parser.add_argument('-o', '--output', metavar='<dir>', default=op.normpath(os.getcwd() + '/output'),
						help='Output dir for the generated files. Defaults to ./output')
	parser.add_argument('-r', '--regex', metavar='<regex>', nargs='+', default=[r".*\.(css|js|html|rb|php)$"],
						help='Add a regex, or list of regexes, to match files to watch for changes. \
								Defaults to \".*\\.(css|js|html|rb|php)$\"')
	parser.add_argument('fetch', metavar='<uri>', nargs='+',
						help='Add uris to fetch changes from.')


	args = parser.parse_args()

	httpman = Http()
	webgetter = WebGetter(args, httpman)
	eventh = EventHandler(webgetter, args)

	observer = Observer()
	observer.schedule(eventh, op.abspath(args.input), recursive=True)
	observer.start()

	try:
		while True:
			time.sleep(1)
	except KeyboardInterrupt:
		print('\nCleaning up...')
		observer.stop()

	observer.join()
